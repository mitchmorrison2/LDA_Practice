{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Practicing an learning LDA topic modeling by following Susan Li's \n",
    "\"Topic Modeling and Latent Dirichlet Allocation (LDA) in Python\" \n",
    " at https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186018\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n",
      "5                  ambitious olsson wins triple jump      5\n",
      "6         antic delighted with record breaking barca      6\n",
      "7  aussie qualifier stosur wastes four memphis match      7\n",
      "8       aust addresses un security council over iraq      8\n",
      "9         australia is locked into war timetable opp      9\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Mitchellmorrison/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original doucment\n",
      "['greens', 'divided', 'over', 'preference', 'distribution']\n",
      "\n",
      "\n",
      " tokenize and lemmatized document: \n",
      "['green', 'divid', 'prefer', 'distribut']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4010].values[0][0]\n",
    "\n",
    "print('original doucment')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenize and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = []\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any words w less than 15 instances overall, best 100000 words\n",
    "dictionary.filter_extremes(no_below=15, no_above=.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(234, 1), (2283, 1), (2743, 1), (3804, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 234 (\"green\") appears 1 times.\n",
      "Word 2283 (\"divid\") appears 1 times.\n",
      "Word 2743 (\"prefer\") appears 1 times.\n",
      "Word 3804 (\"distribut\") appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# frequency of each word in headline for specific headline\n",
    "\n",
    "bow_doc_4010 = bow_corpus[4010]\n",
    "\n",
    "for i in range(len(bow_doc_4010)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} times.\".format\n",
    "          (bow_doc_4010[i][0], \n",
    "           dictionary[bow_doc_4010[i][0]], \n",
    "           bow_doc_4010[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5860586362613176), (1, 0.3854657616151764), (2, 0.5006618583937537), (3, 0.5072367544211179)]\n"
     ]
    }
   ],
   "source": [
    "#get tf/idf ratios for each word\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda using bag of words\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"market\" + 0.022*\"school\" + 0.016*\"investig\" + 0.015*\"interview\" + 0.014*\"street\" + 0.014*\"fall\" + 0.013*\"student\" + 0.012*\"scott\" + 0.011*\"share\" + 0.010*\"week\"\n",
      "Topic: 1 \n",
      "Words: 0.020*\"donald\" + 0.018*\"plan\" + 0.018*\"canberra\" + 0.013*\"council\" + 0.011*\"water\" + 0.011*\"feder\" + 0.011*\"drum\" + 0.010*\"industri\" + 0.010*\"concern\" + 0.010*\"park\"\n",
      "Topic: 2 \n",
      "Words: 0.042*\"australia\" + 0.037*\"trump\" + 0.024*\"world\" + 0.016*\"open\" + 0.015*\"tasmania\" + 0.014*\"final\" + 0.014*\"australian\" + 0.012*\"win\" + 0.011*\"break\" + 0.010*\"record\"\n",
      "Topic: 3 \n",
      "Words: 0.029*\"queensland\" + 0.021*\"help\" + 0.017*\"miss\" + 0.016*\"royal\" + 0.015*\"tasmanian\" + 0.014*\"bank\" + 0.013*\"victoria\" + 0.013*\"report\" + 0.013*\"commiss\" + 0.011*\"drought\"\n",
      "Topic: 4 \n",
      "Words: 0.024*\"news\" + 0.022*\"women\" + 0.021*\"live\" + 0.020*\"warn\" + 0.020*\"coast\" + 0.019*\"health\" + 0.016*\"rural\" + 0.015*\"busi\" + 0.015*\"countri\" + 0.013*\"gold\"\n",
      "Topic: 5 \n",
      "Words: 0.053*\"polic\" + 0.049*\"say\" + 0.024*\"kill\" + 0.023*\"attack\" + 0.021*\"crash\" + 0.018*\"die\" + 0.017*\"shoot\" + 0.017*\"perth\" + 0.014*\"woman\" + 0.013*\"arrest\"\n",
      "Topic: 6 \n",
      "Words: 0.014*\"show\" + 0.013*\"liber\" + 0.013*\"power\" + 0.013*\"morrison\" + 0.012*\"wall\" + 0.012*\"young\" + 0.011*\"christma\" + 0.011*\"station\" + 0.010*\"beach\" + 0.010*\"train\"\n",
      "Topic: 7 \n",
      "Words: 0.035*\"elect\" + 0.031*\"govern\" + 0.024*\"china\" + 0.015*\"rise\" + 0.014*\"australia\" + 0.012*\"million\" + 0.011*\"citi\" + 0.011*\"presid\" + 0.011*\"bushfir\" + 0.011*\"trade\"\n",
      "Topic: 8 \n",
      "Words: 0.034*\"year\" + 0.031*\"charg\" + 0.027*\"court\" + 0.025*\"murder\" + 0.018*\"face\" + 0.017*\"jail\" + 0.016*\"accus\" + 0.015*\"death\" + 0.014*\"peopl\" + 0.013*\"trial\"\n",
      "Topic: 9 \n",
      "Words: 0.020*\"south\" + 0.019*\"adelaid\" + 0.018*\"north\" + 0.014*\"labor\" + 0.011*\"west\" + 0.011*\"worker\" + 0.010*\"stori\" + 0.010*\"state\" + 0.010*\"hospit\" + 0.010*\"polit\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.007*\"weather\" + 0.006*\"water\" + 0.006*\"govern\" + 0.006*\"bushfir\" + 0.006*\"andrew\" + 0.005*\"hobart\" + 0.005*\"council\" + 0.005*\"action\" + 0.005*\"concern\" + 0.005*\"plan\"\n",
      "Topic: 1 \n",
      "Word: 0.019*\"polic\" + 0.018*\"charg\" + 0.016*\"murder\" + 0.012*\"court\" + 0.012*\"woman\" + 0.011*\"death\" + 0.010*\"jail\" + 0.010*\"alleg\" + 0.009*\"arrest\" + 0.009*\"sentenc\"\n",
      "Topic: 2 \n",
      "Word: 0.022*\"trump\" + 0.013*\"interview\" + 0.008*\"final\" + 0.007*\"leagu\" + 0.007*\"world\" + 0.007*\"australia\" + 0.006*\"michael\" + 0.006*\"scott\" + 0.006*\"morrison\" + 0.006*\"grandstand\"\n",
      "Topic: 3 \n",
      "Word: 0.015*\"elect\" + 0.011*\"stori\" + 0.008*\"wall\" + 0.008*\"liber\" + 0.008*\"street\" + 0.007*\"labor\" + 0.007*\"parti\" + 0.006*\"polit\" + 0.006*\"retir\" + 0.006*\"octob\"\n",
      "Topic: 4 \n",
      "Word: 0.013*\"market\" + 0.009*\"price\" + 0.007*\"share\" + 0.007*\"australian\" + 0.007*\"rise\" + 0.007*\"cattl\" + 0.007*\"rat\" + 0.006*\"live\" + 0.006*\"peter\" + 0.006*\"financ\"\n",
      "Topic: 5 \n",
      "Word: 0.026*\"news\" + 0.018*\"rural\" + 0.016*\"drum\" + 0.010*\"tuesday\" + 0.010*\"friday\" + 0.010*\"thursday\" + 0.009*\"video\" + 0.009*\"nation\" + 0.008*\"market\" + 0.007*\"busi\"\n",
      "Topic: 6 \n",
      "Word: 0.018*\"countri\" + 0.015*\"donald\" + 0.014*\"hour\" + 0.007*\"pacif\" + 0.007*\"christma\" + 0.006*\"zealand\" + 0.006*\"august\" + 0.006*\"dairi\" + 0.005*\"food\" + 0.005*\"island\"\n",
      "Topic: 7 \n",
      "Word: 0.011*\"royal\" + 0.009*\"climat\" + 0.008*\"commiss\" + 0.008*\"health\" + 0.007*\"violenc\" + 0.007*\"david\" + 0.007*\"sport\" + 0.007*\"care\" + 0.007*\"juli\" + 0.007*\"mental\"\n",
      "Topic: 8 \n",
      "Word: 0.016*\"crash\" + 0.014*\"kill\" + 0.010*\"dead\" + 0.009*\"wednesday\" + 0.009*\"die\" + 0.008*\"girl\" + 0.007*\"truck\" + 0.006*\"septemb\" + 0.006*\"attack\" + 0.006*\"plane\"\n",
      "Topic: 9 \n",
      "Word: 0.015*\"coast\" + 0.012*\"north\" + 0.011*\"gold\" + 0.009*\"turnbul\" + 0.008*\"korea\" + 0.008*\"south\" + 0.006*\"australia\" + 0.006*\"syria\" + 0.006*\"malcolm\" + 0.005*\"farm\"\n"
     ]
    }
   ],
   "source": [
    "# lda using tf/idf\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWord: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
